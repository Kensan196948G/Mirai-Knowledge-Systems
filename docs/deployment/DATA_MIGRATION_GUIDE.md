# ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã‚¬ã‚¤ãƒ‰ - æœ¬ç•ªç’°å¢ƒå¯¾å¿œ

## ğŸ“‹ æ¦‚è¦

Mirai Knowledge Systemsã¸ã®ãƒ‡ãƒ¼ã‚¿ç§»è¡Œæ‰‹é †ã‚’èª¬æ˜ã—ã¾ã™ã€‚ä»¥ä¸‹ã®3ã¤ã®ç§»è¡ŒçµŒè·¯ã«å¯¾å¿œã—ã¾ã™ï¼š

1. **ç¾è¡Œã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®ç§»è¡Œ** - æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹/ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç§»è¡Œ
2. **Microsoft 365ã‹ã‚‰ã®ç§»è¡Œ** - SharePoint/OneDrive/Teamsç­‰ã‹ã‚‰ç§»è¡Œ
3. **æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿ä½œæˆ** - æ–°è¦ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ

---

## ğŸ¯ ç§»è¡Œæˆ¦ç•¥

### ç§»è¡Œãƒ•ã‚§ãƒ¼ã‚º

| ãƒ•ã‚§ãƒ¼ã‚º | å†…å®¹ | æ‰€è¦æ™‚é–“ | æ‹…å½“ |
|---------|------|---------|------|
| Phase 1 | ãƒ‡ãƒ¼ã‚¿æ£šå¸ãƒ»åˆ†æ | 2-3æ—¥ | ãƒ‡ãƒ¼ã‚¿ç®¡ç†è€… |
| Phase 2 | ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆ | 3-5æ—¥ | é–‹ç™ºè€… |
| Phase 3 | ãƒªãƒãƒ¼ã‚µãƒ«ï¼ˆãƒ†ã‚¹ãƒˆç’°å¢ƒï¼‰ | 1-2æ—¥ | é–‹ç™ºè€…ãƒ»é‹ç”¨è€… |
| Phase 4 | æœ¬ç•ªç§»è¡Œ | 4-8æ™‚é–“ | å…¨å“¡ |
| Phase 5 | æ¤œè¨¼ãƒ»å®‰å®šåŒ– | 1-2æ—¥ | å…¨å“¡ |

---

## ğŸ“ Method 1: ç¾è¡Œã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®ç§»è¡Œ

### å‰ææ¡ä»¶

- ç¾è¡Œã‚µãƒ¼ãƒãƒ¼ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ï¼ˆSSHã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šï¼‰
- ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®æŠŠæ¡ï¼ˆCSVã€Excelã€SQLã€JSONç­‰ï¼‰
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¸¯åŸŸã®ç¢ºä¿

### ã‚¹ãƒ†ãƒƒãƒ—1: ç¾è¡Œãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

#### SQL Serverã‹ã‚‰ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

```bash
# SQL Serverãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# æ–¹æ³•1: bcp ã‚³ãƒãƒ³ãƒ‰
bcp "SELECT * FROM knowledge_table" queryout knowledge_export.csv -c -t, -S servername -U username -P password

# æ–¹æ³•2: SQL Server Management Studio (SSMS)
# ã‚¿ã‚¹ã‚¯ â†’ ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ â†’ ãƒ•ãƒ©ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼

# æ–¹æ³•3: PowerShell
$conn = New-Object System.Data.SqlClient.SqlConnection("Server=servername;Database=dbname;User Id=username;Password=password;")
$cmd = New-Object System.Data.SqlClient.SqlCommand("SELECT * FROM knowledge_table", $conn)
$adapter = New-Object System.Data.SqlClient.SqlDataAdapter($cmd)
$dataset = New-Object System.Data.DataSet
$adapter.Fill($dataset)
$dataset.Tables[0] | Export-Csv -Path "knowledge_export.csv" -NoTypeInformation
```

#### MySQLã‹ã‚‰ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

```bash
# MySQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
mysqldump -u username -p --databases knowledge_db --result-file=knowledge_dump.sql

# ç‰¹å®šãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿
mysqldump -u username -p knowledge_db knowledge_table sop_table > tables_export.sql

# CSVå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
mysql -u username -p -e "SELECT * FROM knowledge_table INTO OUTFILE '/tmp/knowledge.csv' FIELDS TERMINATED BY ',' ENCLOSED BY '\"' LINES TERMINATED BY '\n';" knowledge_db
```

#### PostgreSQLã‹ã‚‰ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

```bash
# ä»–ã®PostgreSQLã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰
pg_dump -h source_server -U username -d source_db -t knowledge_table -f knowledge.sql

# CSVå½¢å¼
psql -h source_server -U username -d source_db -c "\COPY knowledge_table TO 'knowledge.csv' CSV HEADER"
```

#### ãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

```bash
# ãƒ•ã‚¡ã‚¤ãƒ«å…±æœ‰ã‹ã‚‰ã‚³ãƒ”ãƒ¼
scp user@server:/path/to/documents/*.pdf ./migration_data/documents/

# rsyncï¼ˆå·®åˆ†åŒæœŸï¼‰
rsync -avz --progress user@server:/path/to/documents/ ./migration_data/documents/
```

### ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ

```python
#!/usr/bin/env python3
"""
ç¾è¡Œã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’ Mirai Knowledge Systems å½¢å¼ã«å¤‰æ›

ä½¿ç”¨ä¾‹:
    python convert_legacy_data.py --input legacy_export.csv --output knowledge.json
"""

import csv
import json
from datetime import datetime

def convert_csv_to_knowledge(csv_path, output_path):
    """CSVã‚’knowledge.jsonå½¢å¼ã«å¤‰æ›"""
    knowledge_list = []

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)

        for row in reader:
            knowledge = {
                'id': int(row['ID']),
                'title': row['Title'],
                'summary': row['Summary'] or row['Title'][:100],
                'content': row['Content'],
                'category': map_category(row['Category']),  # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°
                'tags': row['Tags'].split(',') if row.get('Tags') else [],
                'status': 'approved',
                'priority': map_priority(row.get('Priority', 'Medium')),
                'project': row.get('Project', ''),
                'owner': row.get('Owner', 'æŠ€è¡“éƒ¨'),
                'created_at': parse_date(row.get('CreatedDate')),
                'updated_at': parse_date(row.get('UpdatedDate'))
            }
            knowledge_list.append(knowledge)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(knowledge_list, f, ensure_ascii=False, indent=2)

    print(f"âœ… {len(knowledge_list)}ä»¶ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã—ã¾ã—ãŸ: {output_path}")

def map_category(legacy_category):
    """ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°"""
    mapping = {
        'æ–½å·¥': 'æ–½å·¥è¨ˆç”»',
        'å“è³ª': 'å“è³ªç®¡ç†',
        'å®‰å…¨': 'å®‰å…¨è¡›ç”Ÿ',
        'QC': 'å“è³ªç®¡ç†',
        'Safety': 'å®‰å…¨è¡›ç”Ÿ',
        # ... ä»–ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    }
    return mapping.get(legacy_category, legacy_category)

def map_priority(legacy_priority):
    """å„ªå…ˆåº¦ãƒãƒƒãƒ”ãƒ³ã‚°"""
    mapping = {
        'High': 'high',
        'Medium': 'medium',
        'Low': 'low',
        'é«˜': 'high',
        'ä¸­': 'medium',
        'ä½': 'low',
    }
    return mapping.get(legacy_priority, 'medium')

def parse_date(date_str):
    """æ—¥ä»˜æ–‡å­—åˆ—ã‚’ISOå½¢å¼ã«å¤‰æ›"""
    if not date_str:
        return datetime.now().isoformat()

    # è¤‡æ•°ã®æ—¥ä»˜å½¢å¼ã«å¯¾å¿œ
    for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%m/%d/%Y', '%d/%m/%Y']:
        try:
            return datetime.strptime(date_str, fmt).isoformat()
        except ValueError:
            continue

    return datetime.now().isoformat()
```

### ã‚¹ãƒ†ãƒƒãƒ—3: Mirai Knowledge Systemsã¸ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

```bash
# 1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
cd /mnt/LinuxHDD/Mirai-Knowledge-Systems/backend
python scripts/backup.sh

# 2. æ—¢å­˜ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
python scripts/clear_dummy_data.py --all

# 3. å¤‰æ›æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLã«æŠ•å…¥
python migrate_json_to_postgres.py

# 4. æ¤œè¨¼
python scripts/verify_migration.py
```

---

## ğŸŒ Method 2: Microsoft 365ã‹ã‚‰ã®ç§»è¡Œ

### å‰ææ¡ä»¶

- Azure ADã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç™»éŒ²
- Microsoft Graph APIã‚¢ã‚¯ã‚»ã‚¹è¨±å¯è¨­å®š
- Client IDã€Client Secretã€Tenant IDã®å–å¾—

### ã‚¹ãƒ†ãƒƒãƒ—1: Azure ADã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç™»éŒ²

```bash
# Azure CLIã§ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç™»éŒ²
az ad app create --display-name "Mirai Knowledge Migration Tool"

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³IDã‚’å–å¾—
APP_ID=$(az ad app list --display-name "Mirai Knowledge Migration Tool" --query "[0].appId" -o tsv)

# Client Secretã‚’ä½œæˆ
az ad app credential reset --id $APP_ID

# APIã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ã‚’è¿½åŠ 
az ad app permission add --id $APP_ID --api 00000003-0000-0000-c000-000000000000 --api-permissions \
    df021288-bdef-4463-88db-98f22de89214=Role \  # Files.Read.All
    7ab1d382-f21e-4acd-a863-ba3e13f7da61=Role     # Directory.Read.All
```

### ã‚¹ãƒ†ãƒƒãƒ—2: Microsoft Graph APIçµŒç”±ã§ãƒ‡ãƒ¼ã‚¿å–å¾—

```python
#!/usr/bin/env python3
"""
Microsoft 365ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦Miraiå½¢å¼ã«å¤‰æ›

ç’°å¢ƒå¤‰æ•°:
    MS365_TENANT_ID: Azure AD Tenant ID
    MS365_CLIENT_ID: Application (client) ID
    MS365_CLIENT_SECRET: Client secret value
"""

import os
import json
from datetime import datetime
from azure.identity import ClientSecretCredential
from msgraph import GraphServiceClient
from msgraph.generated.models.drive_item import DriveItem

# èªè¨¼è¨­å®š
TENANT_ID = os.environ['MS365_TENANT_ID']
CLIENT_ID = os.environ['MS365_CLIENT_ID']
CLIENT_SECRET = os.environ['MS365_CLIENT_SECRET']

# éå¯¾è©±å‹èªè¨¼ï¼ˆclient_credentials ãƒ•ãƒ­ãƒ¼ï¼‰
credential = ClientSecretCredential(
    tenant_id=TENANT_ID,
    client_id=CLIENT_ID,
    client_secret=CLIENT_SECRET
)

client = GraphServiceClient(credentials=credential)

async def get_sharepoint_documents(site_id, drive_id):
    """SharePointã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—"""
    # ãƒ‰ãƒ©ã‚¤ãƒ–ã®ãƒ«ãƒ¼ãƒˆã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
    result = await client.drives.by_drive_id(drive_id).root.children.get()

    knowledge_list = []

    for item in result.value:
        if item.file:  # ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’å–å¾—
            content = await client.drives.by_drive_id(drive_id).items.by_drive_item_id(item.id).content.get()

            knowledge = {
                'title': item.name,
                'summary': item.description or item.name,
                'content': content.decode('utf-8') if isinstance(content, bytes) else str(content),
                'category': parse_category_from_path(item.parent_reference.path),
                'tags': extract_tags_from_metadata(item),
                'status': 'approved',
                'owner': item.created_by.user.display_name,
                'created_at': item.created_date_time.isoformat(),
                'updated_at': item.last_modified_date_time.isoformat()
            }
            knowledge_list.append(knowledge)

    return knowledge_list

async def get_teams_wiki(team_id):
    """Microsoft Teamsã®Wikiãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
    # Teamsã®ãƒãƒ£ãƒãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
    channels = await client.teams.by_team_id(team_id).channels.get()

    knowledge_list = []

    for channel in channels.value:
        # ãƒãƒ£ãƒãƒ«ã®Wikiã‚¿ãƒ–ã‚’å–å¾—
        tabs = await client.teams.by_team_id(team_id).channels.by_channel_id(channel.id).tabs.get()

        for tab in tabs.value:
            if tab.teamsapp.id == 'com.microsoft.teamspace.tab.wiki':
                # Wikiã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
                # ... å¤‰æ›å‡¦ç†
                pass

    return knowledge_list

def parse_category_from_path(path):
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨æ¸¬"""
    if '/æ–½å·¥/' in path:
        return 'æ–½å·¥è¨ˆç”»'
    elif '/å“è³ª/' in path:
        return 'å“è³ªç®¡ç†'
    elif '/å®‰å…¨/' in path:
        return 'å®‰å…¨è¡›ç”Ÿ'
    # ... ä»–ã®ã‚«ãƒ†ã‚´ãƒª
    return 'æœªåˆ†é¡'

def extract_tags_from_metadata(item):
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¿ã‚°ã‚’æŠ½å‡º"""
    tags = []

    # SharePointã®ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    if hasattr(item, 'list_item') and item.list_item:
        fields = item.list_item.fields
        if 'Tags' in fields:
            tags = fields['Tags'].split(';')

    return tags

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
async def main():
    # SharePointã‚µã‚¤ãƒˆIDï¼ˆäº‹å‰ã«å–å¾—ï¼‰
    SITE_ID = "your-site-id"
    DRIVE_ID = "your-drive-id"

    # SharePointãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—
    documents = await get_sharepoint_documents(SITE_ID, DRIVE_ID)

    # JSONå‡ºåŠ›
    with open('ms365_knowledge_export.json', 'w', encoding='utf-8') as f:
        json.dump(documents, f, ensure_ascii=False, indent=2)

    print(f"âœ… {len(documents)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")

if __name__ == '__main__':
    import asyncio
    asyncio.run(main())
```

### ã‚¹ãƒ†ãƒƒãƒ—3: å¿…è¦ãªPythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# Microsoft Graph APIç”¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
pip install azure-identity msgraph-sdk

# ã¾ãŸã¯ requirements.txt ã«è¿½åŠ æ¸ˆã¿
# azure-identity>=1.15.0
# msgraph-sdk>=1.0.0
```

### ã‚¹ãƒ†ãƒƒãƒ—4: å®Ÿè¡Œæ‰‹é †

```bash
# 1. ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
export MS365_TENANT_ID="your-tenant-id"
export MS365_CLIENT_ID="your-client-id"
export MS365_CLIENT_SECRET="your-client-secret"

# 2. ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
python scripts/export_from_ms365.py

# 3. ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
python scripts/validate_migration_data.py --input ms365_knowledge_export.json

# 4. PostgreSQLã«æŠ•å…¥
python migrate_json_to_postgres.py --input ms365_knowledge_export.json

# 5. æ¤œè¨¼
python scripts/verify_migration.py
```

---

## ğŸ› ï¸ Method 3: æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿ä½œæˆ

### CSV ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

`knowledge_template.csv`:

```csv
ID,Title,Summary,Content,Category,Tags,Status,Priority,Project,Owner,CreatedDate,UpdatedDate
1,"ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆæ‰“è¨­æ‰‹é †","å¯’å†·åœ°ã§ã®æ‰“è¨­ç®¡ç†","ã€ç›®çš„ã€‘å¯’å†·æœŸã«ãŠã‘ã‚‹ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆæ‰“è¨­...","æ–½å·¥è¨ˆç”»","ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ,å“è³ªç®¡ç†","approved","high","æ©‹æ¢è£œä¿®","æŠ€è¡“éƒ¨","2026-01-01","2026-01-08"
2,"å®‰å…¨ç¢ºèªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ","æ—¥æ¬¡å®‰å…¨ç¢ºèªé …ç›®","ã€é©ç”¨ç¯„å›²ã€‘å…¨ç¾å ´...","å®‰å…¨è¡›ç”Ÿ","å®‰å…¨,ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ","approved","high","","å®‰å…¨ç®¡ç†å®¤","2026-01-05","2026-01-08"
```

### Excel â†’ JSON å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
#!/usr/bin/env python3
"""
Excelï¼ˆ.xlsxï¼‰ã‚’JSONå½¢å¼ã«å¤‰æ›

ä½¿ç”¨ä¾‹:
    python scripts/excel_to_json.py --input knowledge_data.xlsx --output knowledge.json --sheet "ãƒŠãƒ¬ãƒƒã‚¸ä¸€è¦§"
"""

import openpyxl
import json
import argparse
from datetime import datetime

def excel_to_json(excel_path, output_path, sheet_name='Sheet1'):
    """Excelâ†’JSONå¤‰æ›"""
    workbook = openpyxl.load_workbook(excel_path)
    sheet = workbook[sheet_name]

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’å–å¾—
    headers = [cell.value for cell in sheet[1]]

    knowledge_list = []

    for row in sheet.iter_rows(min_row=2, values_only=True):
        knowledge = {}
        for header, value in zip(headers, row):
            if header and value is not None:
                # æ—¥ä»˜å‹ã®å‡¦ç†
                if isinstance(value, datetime):
                    knowledge[header] = value.isoformat()
                else:
                    knowledge[header] = value

        if knowledge:  # ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            knowledge_list.append(knowledge)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(knowledge_list, f, ensure_ascii=False, indent=2)

    print(f"âœ… {len(knowledge_list)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã—ã¾ã—ãŸ")
    return knowledge_list

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', required=True, help='Input Excel file')
    parser.add_argument('--output', required=True, help='Output JSON file')
    parser.add_argument('--sheet', default='Sheet1', help='Sheet name')
    args = parser.parse_args()

    excel_to_json(args.input, args.output, args.sheet)
```

---

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°å®šç¾©

### ãƒŠãƒ¬ãƒƒã‚¸ï¼ˆKnowledgeï¼‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°

| Mirai ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ | ç¾è¡Œã‚·ã‚¹ãƒ†ãƒ  | MS365 | å¿…é ˆ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ |
|----------------|------------|-------|------|------------|
| id | ID | - | âœ… | è‡ªå‹•æ¡ç•ª |
| title | Title / ã‚¿ã‚¤ãƒˆãƒ« | Name | âœ… | - |
| summary | Summary / æ¦‚è¦ | Description | âœ… | titleã®å…ˆé ­100æ–‡å­— |
| content | Content / æœ¬æ–‡ | File content | â˜ | summaryã¨åŒã˜ |
| category | Category / åˆ†é¡ | Folder path | âœ… | "æœªåˆ†é¡" |
| tags | Tags / ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ | Metadata.Tags | â˜ | [] |
| status | Status / ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ | - | â˜ | "approved" |
| priority | Priority / å„ªå…ˆåº¦ | - | â˜ | "medium" |
| project | Project / ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ | Site name | â˜ | "" |
| owner | Owner / æ‹…å½“è€… | CreatedBy | âœ… | "æŠ€è¡“éƒ¨" |
| created_at | CreatedDate | CreatedDateTime | â˜ | ç¾åœ¨æ™‚åˆ» |
| updated_at | UpdatedDate | LastModifiedDateTime | â˜ | ç¾åœ¨æ™‚åˆ» |

### SOPï¼ˆæ¨™æº–æ–½å·¥æ‰‹é †ï¼‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°

| Mirai ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ | ç¾è¡Œã‚·ã‚¹ãƒ†ãƒ  | MS365 | å¿…é ˆ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ |
|----------------|------------|-------|------|------------|
| id | ID | - | âœ… | è‡ªå‹•æ¡ç•ª |
| title | Title | Name | âœ… | - |
| category | Category | Folder | âœ… | - |
| version | Version / ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | VersionLabel | âœ… | "1.0" |
| revision_date | RevisionDate | LastModifiedDateTime | âœ… | ç¾åœ¨æ—¥ä»˜ |
| content | Content | File content | âœ… | - |
| status | Status | - | â˜ | "active" |

---

## ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼

### ç§»è¡Œå‰æ¤œè¨¼

```python
#!/usr/bin/env python3
"""
ç§»è¡Œãƒ‡ãƒ¼ã‚¿ã®äº‹å‰æ¤œè¨¼

ãƒã‚§ãƒƒã‚¯é …ç›®:
- å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨
- ãƒ‡ãƒ¼ã‚¿å‹ã®å¦¥å½“æ€§
- æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- é‡è¤‡ID
- å¤–éƒ¨ã‚­ãƒ¼æ•´åˆæ€§
"""

import json

def validate_knowledge_data(json_path):
    """ãƒŠãƒ¬ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    errors = []
    warnings = []
    ids = set()

    for i, item in enumerate(data):
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
        required_fields = ['title', 'summary', 'category', 'owner']
        for field in required_fields:
            if field not in item or not item[field]:
                errors.append(f"è¡Œ{i+1}: å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ '{field}' ãŒæ¬ è½")

        # IDé‡è¤‡ãƒã‚§ãƒƒã‚¯
        if 'id' in item:
            if item['id'] in ids:
                errors.append(f"è¡Œ{i+1}: ID {item['id']} ãŒé‡è¤‡")
            ids.add(item['id'])

        # ã‚¿ã‚¤ãƒˆãƒ«é•·ãƒã‚§ãƒƒã‚¯
        if 'title' in item and len(item['title']) > 500:
            warnings.append(f"è¡Œ{i+1}: ã‚¿ã‚¤ãƒˆãƒ«ãŒ500æ–‡å­—ã‚’è¶…éï¼ˆåˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã™ï¼‰")

        # ã‚«ãƒ†ã‚´ãƒªå€¤ãƒã‚§ãƒƒã‚¯
        valid_categories = ['æ–½å·¥è¨ˆç”»', 'å“è³ªç®¡ç†', 'å®‰å…¨è¡›ç”Ÿ', 'ç’°å¢ƒå¯¾ç­–', 'åŸä¾¡ç®¡ç†', 'å‡ºæ¥å½¢ç®¡ç†', 'è¨­è¨ˆå¤‰æ›´', 'å·¥ç¨‹ç®¡ç†']
        if 'category' in item and item['category'] not in valid_categories:
            warnings.append(f"è¡Œ{i+1}: æœªçŸ¥ã®ã‚«ãƒ†ã‚´ãƒª '{item['category']}'")

    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    print(f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœ: {len(data)}ä»¶")
    print(f"  ã‚¨ãƒ©ãƒ¼: {len(errors)}ä»¶")
    print(f"  è­¦å‘Š: {len(warnings)}ä»¶")

    if errors:
        print("\nã€ã‚¨ãƒ©ãƒ¼ã€‘")
        for error in errors[:10]:  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤º
            print(f"  - {error}")

    if warnings:
        print("\nã€è­¦å‘Šã€‘")
        for warning in warnings[:10]:
            print(f"  - {warning}")

    return len(errors) == 0

if __name__ == '__main__':
    import sys
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python validate_migration_data.py <json_file>")
        sys.exit(1)

    if validate_knowledge_data(sys.argv[1]):
        print("\nâœ… æ¤œè¨¼æˆåŠŸ - ãƒ‡ãƒ¼ã‚¿ç§»è¡Œå¯èƒ½")
        sys.exit(0)
    else:
        print("\nâŒ æ¤œè¨¼å¤±æ•— - ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„")
        sys.exit(1)
```

---

## ğŸ”„ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †

### JSONãƒ¢ãƒ¼ãƒ‰ã¸ã®åˆ‡ã‚Šæˆ»ã—

```bash
# 1. ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢
sudo systemctl stop mirai-knowledge-prod

# 2. ç’°å¢ƒå¤‰æ•°ã‚’å¤‰æ›´
vim backend/.env
# MKS_USE_POSTGRESQL=false ã«å¤‰æ›´

# 3. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢
cp -r backend/data/backup_20260108_120000/* backend/data/

# 4. ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•
sudo systemctl start mirai-knowledge-prod

# 5. å‹•ä½œç¢ºèª
curl http://localhost:5100/api/v1/health
```

### PostgreSQLãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

```bash
# Point-in-Time Recoveryï¼ˆPITRï¼‰
# 1. PostgreSQLåœæ­¢
sudo systemctl stop postgresql

# 2. ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ™‚ç‚¹ã«å¾©å…ƒ
sudo rm -rf /var/lib/postgresql/16/main
sudo cp -r /var/backups/postgresql/20260108_backup /var/lib/postgresql/16/main
sudo chown -R postgres:postgres /var/lib/postgresql/16/main

# 3. PostgreSQLèµ·å‹•
sudo systemctl start postgresql

# 4. æ¤œè¨¼
psql -U postgres -d mirai_knowledge_db -c "SELECT COUNT(*) FROM knowledge;"
```

---

## ğŸ“ ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### ç§»è¡Œå‰

- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å–å¾—ï¼ˆJSONã€PostgreSQLï¼‰
- [ ] ç§»è¡Œãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼å®Œäº†
- [ ] ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ãƒªãƒãƒ¼ã‚µãƒ«å®Ÿæ–½
- [ ] ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †ã®ç¢ºèª
- [ ] é–¢ä¿‚è€…ã¸ã®äº‹å‰é€šçŸ¥
- [ ] ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ™‚é–“ã®ç¢ºä¿

### ç§»è¡Œä¸­

- [ ] ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢
- [ ] ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢å®Ÿè¡Œ
- [ ] ãƒ‡ãƒ¼ã‚¿æŠ•å…¥å®Ÿè¡Œ
- [ ] ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç›£è¦–
- [ ] é€²æ—çŠ¶æ³ã®è¨˜éŒ²

### ç§»è¡Œå¾Œ

- [ ] ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ç¢ºèª
- [ ] ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ç›®è¦–ç¢ºèª
- [ ] APIå‹•ä½œç¢ºèªï¼ˆå…¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼‰
- [ ] ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰å‹•ä½œç¢ºèª
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
- [ ] ãƒ­ã‚°ç¢ºèªï¼ˆã‚¨ãƒ©ãƒ¼ãŒãªã„ã‹ï¼‰

---

## ğŸš€ å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆä¸€è¦§

| ã‚¹ã‚¯ãƒªãƒ—ãƒˆ | ç”¨é€” | å ´æ‰€ |
|-----------|------|------|
| `convert_legacy_data.py` | CSV/SQLâ†’JSONå¤‰æ› | `/backend/scripts/` |
| `export_from_ms365.py` | MS365ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ | `/backend/scripts/` |
| `excel_to_json.py` | Excelâ†’JSONå¤‰æ› | `/backend/scripts/` |
| `validate_migration_data.py` | ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ | `/backend/scripts/` |
| `clear_dummy_data.py` | ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ | `/backend/scripts/` |
| `migrate_json_to_postgres.py` | JSONâ†’PostgreSQL | `/backend/` |
| `import_detailed_data.py` | è©³ç´°ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ | `/backend/` |
| `verify_migration.py` | ç§»è¡Œæ¤œè¨¼ | `/backend/scripts/` |

---

## ğŸ’¡ ãƒ’ãƒ³ãƒˆã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ

- ãƒãƒƒãƒå‡¦ç†: 1000ä»¶ãšã¤ã«åˆ†å‰²ã—ã¦æŠ•å…¥
- é€²æ—è¡¨ç¤º: tqdmãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§é€²æ—ãƒãƒ¼è¡¨ç¤º
- ã‚¨ãƒ©ãƒ¼ãƒªã‚«ãƒãƒª: å¤±æ•—æ™‚ã«é€”ä¸­ã‹ã‚‰å†é–‹å¯èƒ½ã«ã™ã‚‹

### æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

- UTF-8ã«çµ±ä¸€
- Shift_JISã‚„EUC-JPã‹ã‚‰ã®å¤‰æ›ã«æ³¨æ„
- æ–‡å­—åŒ–ã‘ç™ºç”Ÿæ™‚ã¯`chardet`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§è‡ªå‹•æ¤œå‡º

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

- ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ: `db.bulk_insert_mappings()`ä½¿ç”¨
- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€æ™‚ç„¡åŠ¹åŒ–: å¤§é‡æŠ•å…¥æ™‚ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç„¡åŠ¹åŒ–
- ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚º: 500-1000ä»¶ã”ã¨ã«commit

---

## ğŸ“š å‚è€ƒè³‡æ–™

- **Microsoft Graph API**: https://learn.microsoft.com/graph/api/overview
- **Azure ADèªè¨¼**: https://learn.microsoft.com/azure/active-directory/develop/
- **PostgreSQL COPY**: https://www.postgresql.org/docs/current/sql-copy.html
- **SQLAlchemy Bulk Operations**: https://docs.sqlalchemy.org/en/14/orm/persistence_techniques.html#bulk-operations

---

**ä½œæˆæ—¥**: 2026-01-08
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0
**å¯¾è±¡**: Phase C - æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ç§»è¡Œ
